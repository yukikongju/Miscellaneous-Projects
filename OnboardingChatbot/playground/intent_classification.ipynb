{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "- https://pub.aimind.so/fine-tuning-bert-for-intent-classification-from-scratch-7e04be18b733\n",
    "- https://huggingface.co/google-bert/bert-base-uncased\n",
    "- https://huggingface.co/blog/Valerii-Knowledgator/multi-label-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"BuildaByte/Meditation-miniset-v0.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['train'].features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(ds['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['system_prompt'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df['context'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df['suggested_techniques'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "set([t.strip() for s in df['suggested_techniques'].unique() for t in s.split(',')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df['user_prompt'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df['intended_outcome'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df['affirmations_and_mindfulness'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['user_prompt'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Train Model - Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"google-bert/bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"user_prompt\"], \n",
    "        truncation=True, \n",
    "        padding=\"max_length\",  # Ensures all sequences in a batch are of the same length\n",
    "        max_length=128        # Maximum sequence length\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "intended_outcome_dict = {intent: i for i, intent in enumerate(df['intended_outcome'].unique())}\n",
    "intended_outcome_inverse_dict = {i: intent for i, intent in enumerate(df['intended_outcome'].unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_intended_outcome_id(example):\n",
    "    example['intended_outcome_id'] = intended_outcome_dict.get(example['intended_outcome'])\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- map inputs and label\n",
    "tokenized_datasets = ds.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.map(add_intended_outcome_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename column to use Hugging Face Trainer\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"intended_outcome_id\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tokenized_datasets[\"train\"]\n",
    "split_dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "val_split_dataset = split_dataset[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "test_dataset = val_split_dataset[\"train\"]\n",
    "val_dataset = val_split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- training\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(set(train_dataset[\"labels\"])), \n",
    "                                                          problem_type=\"single_label_classification\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",          # output directory\n",
    "    eval_strategy=\"epoch\",    # evaluate at each epoch\n",
    "    learning_rate=5e-5,             # learning rate\n",
    "    per_device_train_batch_size=16, # batch size for training\n",
    "    per_device_eval_batch_size=16,  # batch size for evaluation\n",
    "    num_train_epochs=1,             # number of training epochs\n",
    "    weight_decay=0.01,              # strength of weight decay\n",
    "    logging_dir=\"./logs\",           # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    # logging_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = torch.argmax(torch.tensor(logits), dim=-1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./results\")\n",
    "tokenizer.save_pretrained(\"./results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate(test_dataset)\n",
    "print(\"Test Results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- prediction ----\n",
    "MODEL_NAME = \"./results\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_intent(text, model, tokenizer):\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_class = torch.argmax(logits, dim=-1).item()\n",
    "    return predicted_class\n",
    "\n",
    "\n",
    "text = \"I am stressed out at work.\" # 0\n",
    "text = \"I am anxious at work\" # 3\n",
    "text = \"I am not feeling confident in my skills\" # 4\n",
    "predicted_class = predict_intent(text, model, tokenizer)\n",
    "print(f\"Predicted Intent Class: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intended_outcome_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "### Train Model - Multi-classification model (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- make the multi-class dataset\n",
    "\n",
    "suggested_techniques = set([t.strip() for s in df['suggested_techniques'].unique() for t in s.split(',')])\n",
    "suggested_technique_dict = {t: i for i, t in enumerate(suggested_techniques)}\n",
    "suggested_technique_inverse_dict = {i: t for i, t in enumerate(suggested_techniques)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "suggested_technique_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_suggested_techniques_multi_label(example):\n",
    "    t = torch.tensor([0]*len(suggested_technique_dict), dtype=torch.float64)\n",
    "    for e in [t.strip() for t in example[\"suggested_techniques\"].split(',')]:\n",
    "        idx = suggested_technique_dict.get(e)\n",
    "        t[idx] = 1\n",
    "    example[\"suggested_techniques_multi_label\"] = t\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = ds.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.map(add_suggested_techniques_multi_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_datasets[\"train\"][0][\"suggested_techniques_multi_label\"])\n",
    "print(len(tokenized_datasets[\"train\"][0][\"suggested_techniques_multi_label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- rename column to match HuggingFace Dataset\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"suggested_techniques_multi_label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- split dataset\n",
    "dataset = tokenized_datasets[\"train\"]\n",
    "train_test_split = dataset.train_test_split(train_size=0.8, seed=42)\n",
    "test_val_split = train_test_split[\"test\"].train_test_split(train_size=0.5, seed=42)\n",
    "\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "test_dataset = test_val_split[\"train\"]\n",
    "val_dataset = test_val_split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = tokenizer([\"this movie was great!\", \"it was terrible\"], padding=True, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- training\n",
    "MODEL_NAME = \"google-bert/bert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(suggested_technique_dict), \n",
    "                                                          problem_type=\"multi_label_classification\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",          # output directory\n",
    "    eval_strategy=\"epoch\",    # evaluate at each epoch\n",
    "    learning_rate=5e-5,             # learning rate\n",
    "    per_device_train_batch_size=16, # batch size for training\n",
    "    per_device_eval_batch_size=16,  # batch size for evaluation\n",
    "    num_train_epochs=1,             # number of training epochs\n",
    "    weight_decay=0.01,              # strength of weight decay\n",
    "    logging_dir=\"./logs\",           # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    # logging_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    # metric_for_best_model=\"accuracy\", # need to define later?\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.classifier)\n",
    "print(model.config.num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1 = evaluate.load(\"f1\")\n",
    "# accuracy = evaluate.load(\"accuracy\")\n",
    "# precision = evaluate.load(\"precision\")\n",
    "# recall = evaluate.load(\"recall\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "\n",
    "def sigmoid(x):\n",
    "   return 1/(1 + np.exp(-x))\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = sigmoid(logits)\n",
    "    preds = (probs > 0.5).astype(int).reshape(-1)\n",
    "    labels = labels.astype(int).reshape(-1)\n",
    "\n",
    "    # compute metrics\n",
    "    return clf_metrics.compute(predictions=preds, references=labels)\n",
    "    \n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- save model\n",
    "MUTLICLASS_MODEL_NAME = \"./results_multiclass\"\n",
    "trainer.save_model(MUTLICLASS_MODEL_NAME)\n",
    "tokenizer.save_pretrained(MUTLICLASS_MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MUTLICLASS_MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MUTLICLASS_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate(test_dataset)\n",
    "print(\"Test Results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_multiclass(text, model, tokenizer, THRESHOLD=0.5):\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probs = sigmoid(logits)\n",
    "        preds = (probs > THRESHOLD).int()\n",
    "        \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classes_from_predictions(predicted_classes):\n",
    "    outputs = []\n",
    "    for pred in predicted_classes:\n",
    "        out = [suggested_technique_inverse_dict.get(i) for i, p in enumerate(pred) if p == 1]\n",
    "        outputs.append(out)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- prediction\n",
    "text = \"I am stressed out at work.\" # ['guided imagery', 'deep breathing', 'progressive muscle relaxation']\n",
    "text = \"I am anxious at work\" # ['gentle breathing', 'grounding exercises', 'body scan'\n",
    "text = \"I am not feeling confident in my skills\" # ['gentle breathing', 'grounding exercises', 'body scan']\n",
    "predicted_classes = predict_multiclass(text, model, tokenizer)\n",
    "predicted_classes_names = get_classes_from_predictions(predicted_classes)\n",
    "print(f\"Predicted Multi-Class: {predicted_classes}; Classes Names: {predicted_classes_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
