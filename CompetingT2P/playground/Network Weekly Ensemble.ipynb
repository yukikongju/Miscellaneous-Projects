{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "Improvements from First Try:\n",
    "- Look for leakage => no leakage found\n",
    "- Train with network\n",
    "- Evaluate if model is underfitting/overfitting => it's underfitting\n",
    "- Add additional features => polynomial, interaction effects, dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from mlflow.models import infer_signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "MLFLOW_URI = os.environ.get(\"MLFLOW_URI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 list | grep mlflow \n",
    "!pip3 list | grep pandas \n",
    "!pip3 list | grep scipy \n",
    "!pip3 list | grep numpy \n",
    "!pip3 list | grep statsmodels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow \n",
    "\n",
    "mlflow.set_tracking_uri(MLFLOW_URI)\n",
    "\n",
    "EXPERIMENT_NAME = \"T2P Ensemble\"\n",
    "if not mlflow.get_experiment_by_name(name=EXPERIMENT_NAME):\n",
    "    mlflow.create_experiment(name=EXPERIMENT_NAME)\n",
    "experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/emulie/Documents/poc/T2PArima/data/merged_20250804.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRIAL_COL = 'trials_hauutm'\n",
    "PAID_COL = 'paid_hauutm'\n",
    "\n",
    "valid_country_mask = df['country'].apply(lambda x: isinstance(x, str))\n",
    "zero_country_mask = df['country'] == '0'\n",
    "valid_continent_mask = df['continent'].apply(lambda x: isinstance(x, str))\n",
    "valid_subcontinent_mask = df['sub_continent'].apply(lambda x: isinstance(x, str))\n",
    "\n",
    "df = df[valid_country_mask & ~zero_country_mask & valid_continent_mask & valid_subcontinent_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- minimum conversions required\n",
    "df['t2p'] = df[PAID_COL] / df[TRIAL_COL]\n",
    "\n",
    "min_cost_mask = df['cost_usd'] > 5.0\n",
    "min_paid_mask = df[PAID_COL] > 2.0\n",
    "min_trial_mask = df[TRIAL_COL] > 5.0\n",
    "min_t2p_mask = df['t2p'] > 0\n",
    "df_overall = df[min_cost_mask & min_paid_mask & min_trial_mask & min_t2p_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "T1_countries = [\n",
    "    \"GU\", \"PR\", \"DK\", \"JE\", \"NO\", \"BE\", \"FR\", \"US\", \"IL\", \"GB\", \"UK\",\n",
    "    \"CA\", \"AU\", \"IE\", \"NL\", \"SE\", \"ES\", \"IT\", \"TW\", \"DE\", \"FI\",\n",
    "    \"NZ\", \"JP\", \"KR\", \"SG\", \"HK\"\n",
    "]\n",
    "T2_countries = [\n",
    "    \"ZA\", \"MT\", \"AE\", \"SA\", \"PL\", \"AT\", \"NO\", \"DK\", \"IS\", \"FI\"\n",
    "]\n",
    "T3_countries = [\n",
    "    \"IN\", \"PH\", \"MY\", \"NG\", \"TH\", \"VN\", \"EG\", \"MN\", \"RO\", \"HU\", \"RS\", \"TR\"\n",
    "]\n",
    "\n",
    "\n",
    "country_tier_map = {country: 'T1' for country in T1_countries} | {country: 'T2' for country in T2_countries} | {country: 'T3' for country in T3_countries}\n",
    "df_overall['country_tier'] = df_overall['country'].apply(lambda x: country_tier_map[x] if x in country_tier_map else 'T4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Feature Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- encode categorical columns\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# import joblib\n",
    "\n",
    "# run_name = f\"LABEL_ENCODER_{SEGMENTATION}_{datetime.now().strftime('%Y-%m-%d_%H:%M')}\"\n",
    "# experiment_tags = {\n",
    "#     \"project_name\": EXPERIMENT_NAME, \n",
    "#     \"date\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'), # need to be a string\n",
    "#     \"model\": \"Label Encoder\", \n",
    "#     \"mlflow.note.content\": experiment_description,\n",
    "# }\n",
    "\n",
    "\n",
    "# with mlflow.start_run(experiment_id=experiment.experiment_id, \n",
    "#                                       run_name=run_name, tags=experiment_tags):\n",
    "#     for col in ['network', 'platform', 'country', 'continent', 'sub_continent', 'country_tier']:\n",
    "#         le = LabelEncoder()\n",
    "#         df_overall[f'{col}_encoded'] = le.fit_transform(df_overall[col])\n",
    "#         label_encoder_path = f\"labelencoder_{col}.pkl\"\n",
    "#         joblib.dump(le, label_encoder_path)\n",
    "#         mlflow.log_artifact(label_encoder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overall['network'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_map = {\n",
    "    'Apple Search Ads': 0, \n",
    "    'Facebook Ads': 1, \n",
    "    'googleadwords_int': 2, \n",
    "    'tiktokglobal_int': 3, \n",
    "    'tatari_streaming': 4, \n",
    "    'snapchat_int': 5,\n",
    "    'other': 6,\n",
    "}\n",
    "\n",
    "platform_map = {\n",
    "    'android': 0, \n",
    "    'ios': 1, \n",
    "    'web': 2\n",
    "}\n",
    "\n",
    "country_tier_map = {\n",
    "    'T1': 0, \n",
    "    'T2': 1, \n",
    "    'T3': 2, \n",
    "    'T4': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = df_overall.copy()\n",
    "df_encoded['network'] = df_encoded['network'].apply(lambda x: network_map[x])\n",
    "df_encoded['platform'] = df_encoded['platform'].apply(lambda x: platform_map[x])\n",
    "df_encoded['country_tier'] = df_encoded['country_tier'].apply(lambda x: country_tier_map[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Weekly Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weekly = df_encoded.copy()\n",
    "df_weekly['date'] = pd.to_datetime(df['date'])\n",
    "df_weekly = df_weekly.groupby([pd.Grouper(key='date', freq='W-MON'), 'network', 'platform', 'country', 'week_of_year']).agg({\n",
    "    'cost_usd': 'sum', \n",
    "    'impressions': 'sum', \n",
    "    'clicks': 'sum', \n",
    "    'installs': 'sum', \n",
    "    'trials_hauutm': 'sum', \n",
    "    'paid_hauutm': 'sum', \n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weekly['t2p'] = df_weekly[PAID_COL] / df_weekly[TRIAL_COL]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Training - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = df_weekly.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### 1. GLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "#### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cols = ['network', 'platform', 'week_of_year', 'cost_usd']\n",
    "\n",
    "TRIAL_COL = 'trials_hauutm'\n",
    "PAID_COL = 'paid_hauutm'\n",
    "\n",
    "formula_t2p = f\"t2p ~ {' + '.join(X_cols)}\"\n",
    "cols_to_log_transform = ['cost_usd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "formula_t2p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y_trial, y_paid, y_t2p = dff[X_cols], dff[TRIAL_COL], dff[PAID_COL], dff['t2p']\n",
    "\n",
    "for col in cols_to_log_transform:\n",
    "    X[col] = np.log(X[col])\n",
    "\n",
    "N = int(len(dff) * 0.8)\n",
    "X_train, X_test, y_trial_train, y_trial_test, y_paid_train, y_paid_test = X[:N], X[N:], y_trial[:N], y_trial[N:], y_paid[:N], y_paid[N:]\n",
    "y_t2p_train, y_t2p_test = y_t2p[:N], y_t2p[N:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "model_t2p = sm.GLM(y_t2p_train, X_train, formula=formula_t2p, family=sm.families.Poisson()).fit()\n",
    "glm_predicted = model_t2p.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_glm_t2p = mean_squared_error(glm_predicted, y_t2p_test)\n",
    "print(mse_glm_t2p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_relative_errors = [abs(pred - actual)/actual for pred, actual in zip(glm_predicted, y_t2p_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(glm_relative_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "### 2. XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "#### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cols = ['network', 'platform', 'week_of_year', 'cost_usd', 'impressions', 'clicks', 'installs']\n",
    "TRIAL_COL = 'trials_hauutm'\n",
    "PAID_COL = 'paid_hauutm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y_trial, y_paid, y_t2p = dff[X_cols], dff[TRIAL_COL], dff[PAID_COL], dff['t2p']\n",
    "\n",
    "# N = int(len(dff) * 0.8)\n",
    "# X_train, X_test, y_trial_train, y_trial_test, y_paid_train, y_paid_test = X[:N], X[N:], y_trial[:N], y_trial[N:], y_paid[:N], y_paid[N:]\n",
    "# y_t2p_train, y_t2p_test = y_t2p[:N], y_t2p[N:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# params = {\n",
    "#     \"n_estimators\": 200,\n",
    "#     \"max_depth\": 4,\n",
    "#     \"min_samples_split\": 5,\n",
    "#     \"learning_rate\": 0.05,\n",
    "#     \"loss\": \"squared_error\",\n",
    "# }\n",
    "\n",
    "param_dist = {\n",
    "    \"n_estimators\": randint(50, 300),\n",
    "    \"max_depth\": randint(2, 8),\n",
    "    \"min_samples_split\": randint(2, 10),\n",
    "    \"learning_rate\": uniform(0.005, 0.1),\n",
    "    \"loss\": [\"squared_error\", \"absolute_error\"],\n",
    "}\n",
    "xgb_t2p = GradientBoostingRegressor(random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "n_splits = 5\n",
    "skf = KFold(n_splits=n_splits, shuffle=True, random_state=420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search = RandomizedSearchCV(\n",
    "    xgb_t2p,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    cv=skf,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "random_search.fit(X, y_t2p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_best_model = random_search.best_estimator_\n",
    "print(\"Best parameters:\", random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Evaluate best XGBoost model with cross-validation\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, y_t2p)):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y_t2p.iloc[train_index], y_t2p.iloc[test_index]\n",
    "\n",
    "    # Train model\n",
    "    xgb_best_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict\n",
    "    y_pred_train = xgb_best_model.predict(X_train)\n",
    "    y_pred_test = xgb_best_model.predict(X_test)\n",
    "\n",
    "    # Compute metrics\n",
    "    xgb_mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "    xgb_mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "\n",
    "    xgb_rel_error_train = np.mean(np.abs(y_pred_train - y_train) / y_train)\n",
    "    xgb_rel_error_test = np.mean(np.abs(y_pred_test - y_test) / y_test)\n",
    "\n",
    "    # Output\n",
    "    print(f\"---- Fold {i} ----\")\n",
    "    print(f\"Training => MSE: {xgb_mse_train:.4f}; Mean Relative Error: {xgb_rel_error_train:.4f}\")\n",
    "    print(f\"Testing  => MSE: {xgb_mse_test:.4f}; Mean Relative Error: {xgb_rel_error_test}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "### 3. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "#### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cols = ['network', 'platform', 'week_of_year', 'cost_usd', 'impressions', 'clicks', 'installs']\n",
    "TRIAL_COL = 'trials_hauutm'\n",
    "PAID_COL = 'paid_hauutm'\n",
    "\n",
    "X, y_trial, y_paid, y_t2p = dff[X_cols], dff[TRIAL_COL], dff[PAID_COL], dff['t2p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': randint(50, 300),\n",
    "    'max_depth': randint(3, 20),\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'bootstrap': [True, False],\n",
    "}\n",
    "rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "# note: increasing search space to 3000 and 200 doesn't improve training nor testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=skf,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "random_search.fit(X, y_t2p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf = random_search.best_estimator_\n",
    "print(\"Best parameters found:\\n\", random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "skf = KFold(n_splits=n_splits, shuffle=True, random_state=420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best model with cross-validation\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, y_t2p)):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y_t2p.iloc[train_index], y_t2p.iloc[test_index]\n",
    "\n",
    "    best_rf.fit(X_train, y_train)\n",
    "    y_pred_train = best_rf.predict(X_train)\n",
    "    y_pred_test = best_rf.predict(X_test)\n",
    "\n",
    "    rf_mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "    rf_mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "    rf_rel_error_test = np.mean(np.abs(y_pred_test - y_test) / (y_test))\n",
    "    rf_rel_error_train = np.mean(np.abs(y_pred_train - y_train) / (y_train))\n",
    "\n",
    "    print(f\"---- Fold {i} ----\")\n",
    "    print(f\"Training => MSE: {rf_mse_train:.4f}; Mean Relative Error: {rf_rel_error_train:.4f}\")\n",
    "    print(f\"Training => MSE: {rf_mse_test:.4f}; Mean Relative Error: {rf_rel_error_test:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "Model is underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "### 4. LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "### 5. CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "### 6. MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weekly.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "## --- Weekly Evaluation ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weekly = dff.copy()\n",
    "df_weekly['date'] = pd.to_datetime(df['date'])\n",
    "df_weekly = df_weekly.groupby([pd.Grouper(key='date', freq='W-MON')]).agg({\n",
    "    'cost_usd': 'sum', \n",
    "    'impressions': 'sum', \n",
    "    'clicks': 'sum', \n",
    "    'installs': 'sum', \n",
    "    'trials_hauutm': 'sum', \n",
    "    'paid_hauutm': 'sum', \n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weekly_pred = dff.copy()\n",
    "df_weekly_pred['t2p_predicted'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
