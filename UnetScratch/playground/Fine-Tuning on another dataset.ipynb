{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "62504ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision \n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import ast\n",
    "import os\n",
    "\n",
    "import difflib\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42254b88",
   "metadata": {},
   "source": [
    "### Comparing Classes from Caltech101 dataset and ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5381084c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emulie/pvenv/lib/python3.11/site-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'gist.github.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/Users/emulie/pvenv/lib/python3.11/site-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'gist.githubusercontent.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# --- get dictionary for image net\n",
    "imagenet_dct_url = \"https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a/raw\"\n",
    "resp = requests.get(imagenet_dct_url, verify=False)\n",
    "dct_imagenet = ast.literal_eval(resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "787aedba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a691ba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- get classes names for caltech101\n",
    "with open('../caltech-dct.txt', 'r') as f:\n",
    "    classes_caltech101 = f.readlines()\n",
    "classes_caltech101 = [name.strip().lower() for name in classes_caltech101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ae5aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f48bed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['faces', 'faces_easy', 'leopards', 'motorbikes', 'accordion']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_caltech101[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9929281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tench, tinca tinca',\n",
       " 'goldfish, carassius auratus',\n",
       " 'great white shark, white shark, man-eater, man-eating shark, carcharodon carcharias',\n",
       " 'tiger shark, galeocerdo cuvieri',\n",
       " 'hammerhead, hammerhead shark']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_imagenet = [name.lower() for name in dct_imagenet.values()]\n",
    "classes_imagenet[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f10a1e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- compute overlapping classes\n",
    "classes_imagenet_split = [classe.split(', ') for classe in classes_imagenet]\n",
    "classes_imagenet_split = [[word.replace(' ', '_') for word in classe] for classe in classes_imagenet_split]\n",
    "classes_imagenet_split[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "be2008b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_imagenet_flat_lst = [word for classe in classes_imagenet for word in classe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d4aa293a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tench', 'tinca', 'tinca', 'goldfish', 'carassius']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_imagenet_flat_lst2 = [word for classe in classes_imagenet for word in classe.replace(',', '').split(' ')]\n",
    "classes_imagenet_flat_lst2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14662ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf50c0fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8a37ac2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_classes = []\n",
    "matching_classes = []\n",
    "for caltech101_class in classes_caltech101:\n",
    "    closest_classes = difflib.get_close_matches(caltech101_class, classes_imagenet_flat_lst2, n=1, cutoff=0.8)\n",
    "    \n",
    "    if len(closest_classes) == 0:\n",
    "        missing_classes.append(caltech101_class)\n",
    "    else:\n",
    "        matching_classes.append((caltech101_class, closest_classes))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "969e3237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('faces', ['face']),\n",
       " ('leopards', ['leopard']),\n",
       " ('accordion', ['accordion']),\n",
       " ('airplanes', ['warplane']),\n",
       " ('ant', ['ant']),\n",
       " ('barrel', ['barrel']),\n",
       " ('bass', ['brass']),\n",
       " ('beaver', ['beaver']),\n",
       " ('binocular', ['binoculars']),\n",
       " ('bonsai', ['bonasa']),\n",
       " ('brain', ['brain']),\n",
       " ('butterfly', ['butterfly']),\n",
       " ('camera', ['camera']),\n",
       " ('cannon', ['cannon']),\n",
       " ('cellphone', ['cellphone']),\n",
       " ('chair', ['chair']),\n",
       " ('crab', ['crab']),\n",
       " ('crayfish', ['crayfish']),\n",
       " ('crocodile', ['crocodile']),\n",
       " ('cup', ['cup']),\n",
       " ('dalmatian', ['dalmatian']),\n",
       " ('dragonfly', ['dragonfly']),\n",
       " ('elephant', ['elephant']),\n",
       " ('ewer', ['ewer']),\n",
       " ('ferry', ['kerry']),\n",
       " ('flamingo', ['flamingo']),\n",
       " ('hedgehog', ['hedgehog']),\n",
       " ('kangaroo', ['kangaroo']),\n",
       " ('lamp', ['lamp']),\n",
       " ('laptop', ['laptop']),\n",
       " ('llama', ['llama']),\n",
       " ('lobster', ['lobster']),\n",
       " ('nautilus', ['nautilus']),\n",
       " ('panda', ['panda']),\n",
       " ('pizza', ['pizza']),\n",
       " ('platypus', ['platypus']),\n",
       " ('revolver', ['revolver']),\n",
       " ('saxophone', ['saxophone']),\n",
       " ('schooner', ['schooner']),\n",
       " ('scorpion', ['scorpion']),\n",
       " ('sea_horse', ['seashore']),\n",
       " ('stapler', ['taper']),\n",
       " ('starfish', ['starfish']),\n",
       " ('strawberry', ['strawberry']),\n",
       " ('tick', ['tick']),\n",
       " ('trilobite', ['trilobite']),\n",
       " ('umbrella', ['umbrella']),\n",
       " ('watch', ['watch']),\n",
       " ('wrench', ['trench'])]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matching_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4078d952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['faces_easy',\n",
       " 'motorbikes',\n",
       " 'anchor',\n",
       " 'brontosaurus',\n",
       " 'buddha',\n",
       " 'car_side',\n",
       " 'ceiling_fan',\n",
       " 'chandelier',\n",
       " 'cougar_body',\n",
       " 'cougar_face',\n",
       " 'crocodile_head',\n",
       " 'dollar_bill',\n",
       " 'dolphin',\n",
       " 'electric_guitar',\n",
       " 'emu',\n",
       " 'euphonium',\n",
       " 'flamingo_head',\n",
       " 'garfield',\n",
       " 'gerenuk',\n",
       " 'gramophone',\n",
       " 'grand_piano',\n",
       " 'hawksbill',\n",
       " 'headphone',\n",
       " 'helicopter',\n",
       " 'ibis',\n",
       " 'inline_skate',\n",
       " 'joshua_tree',\n",
       " 'ketch',\n",
       " 'lotus',\n",
       " 'mandolin',\n",
       " 'mayfly',\n",
       " 'menorah',\n",
       " 'metronome',\n",
       " 'minaret',\n",
       " 'octopus',\n",
       " 'okapi',\n",
       " 'pagoda',\n",
       " 'pigeon',\n",
       " 'pyramid',\n",
       " 'rhino',\n",
       " 'rooster',\n",
       " 'scissors',\n",
       " 'snoopy',\n",
       " 'soccer_ball',\n",
       " 'stegosaurus',\n",
       " 'stop_sign',\n",
       " 'sunflower',\n",
       " 'water_lilly',\n",
       " 'wheelchair',\n",
       " 'wild_cat',\n",
       " 'windsor_chair',\n",
       " 'yin_yang']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bd1d8e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing classes: 51.485149% (52/101)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Missing classes: {len(missing_classes) / len(classes_caltech101) * 100:2f}% ({len(missing_classes)}/{len(classes_caltech101)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc7ce99",
   "metadata": {},
   "source": [
    "### Load CalTech101 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a0243596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/imbikramsaha/caltech-101?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 131M/131M [00:04<00:00, 27.5MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: download data from kaggle https://www.kaggle.com/datasets/imbikramsaha/caltech-101\n",
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"imbikramsaha/caltech-101\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "66bdcc0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/emulie/.cache/kagglehub/datasets/imbikramsaha/caltech-101/versions/1'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b9885351",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Caltech101Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset_dir: int, split: str, transform=None, train_split: float = 0.8):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "        self.train_split = train_split\n",
    "        \n",
    "        self.classes = os.listdir(dataset_dir)\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "        self.images = self._load_images()\n",
    "    \n",
    "    def _load_images(self):\n",
    "        images = []\n",
    "        for cls in self.classes:\n",
    "            class_dir = os.path.join(self.dataset_dir, cls)\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                img_path = os.path.join(class_dir, img_name)\n",
    "                images.append((img_path, self.class_to_idx[cls]))\n",
    "                \n",
    "        random.shuffle(images)\n",
    "        \n",
    "        e_train = int(len(images) * self.train_split)\n",
    "        if self.split == \"train\":\n",
    "            images = images[:e_train]\n",
    "        elif self.split == \"test\":\n",
    "            images = images[e_train:]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid split value. Specify 'train' or 'test' \")\n",
    "        \n",
    "        return images\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.images[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = image.resize((64, 64))\n",
    "            image = np.asarray(image)\n",
    "            image = np.transpose(image, (2, 0, 1))\n",
    "            image = torch.tensor(image, dtype=torch.float32)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "431261de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = path + '/caltech-101/'\n",
    "train_dataset = Caltech101Dataset(dataset_dir, split='train')\n",
    "test_dataset = Caltech101Dataset(dataset_dir, split='test')\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "9c029b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TODO: visualize image \n",
    "batch, labels = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "dfb3435d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [4.0..255.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.0..134.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [7.0..255.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [14.0..220.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.0..255.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.0..255.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.0..255.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.0..255.0].\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGfCAYAAAAZGgYhAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAcyElEQVR4nO3df2xV9f3H8Vex7aUC95ZWuG1Hy2pECyIMC5Q7cN8MOhtiDIzq0GDGHJHICgpolCYT3OIs0TgVxw91DlwmMlmCigkwUrVOVxCqRJRZQZu1s9yLLvbe0tkLoZ/vH8abVcrkllvevZfnIzkJPefc2/cnJPeZc3tum+accwIA4DwbYD0AAODCRIAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAm0vvqideuXauHH35YwWBQ48eP1xNPPKHJkyd/6+O6urrU2tqqIUOGKC0tra/GAwD0Eeec2tvbVVBQoAED/sd1jusDW7ZscZmZme4Pf/iD++CDD9xtt93msrOzXSgU+tbHtrS0OElsbGxsbEm+tbS0/M/X+zTnEv/LSMvKyjRp0iT97ne/k/TVVU1hYaGWLFmiFStW/M/HhsNhZWdnq6WlRV6vN9GjAQD6WCQSUWFhodra2uTz+c54XsLfgjtx4oQaGhpUXV0d2zdgwACVl5ervr7+tPOj0aii0Wjs6/b2dkmS1+slQACQxL7txygJvwnh888/16lTp+T3+7vt9/v9CgaDp51fU1Mjn88X2woLCxM9EgCgHzK/C666ulrhcDi2tbS0WI8EADgPEv4W3CWXXKKLLrpIoVCo2/5QKKS8vLzTzvd4PPJ4PIkeAwDQzyX8CigzM1OlpaWqra2N7evq6lJtba0CgUCivx0AIEn1yeeAli9frvnz52vixImaPHmyHnvsMXV0dOjWW2/ti28HAEhCfRKguXPn6rPPPtPKlSsVDAb1ve99Tzt37jztxgQAwIWrTz4HdC4ikYh8Pp/C4TC3YQNAEjrb13Hzu+AAABcmAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGAi7gC98cYbuv7661VQUKC0tDS9+OKL3Y4757Ry5Url5+crKytL5eXlOnz4cKLmBQCkiLgD1NHRofHjx2vt2rU9Hn/ooYe0Zs0abdiwQXv37tWgQYNUUVGhzs7Ocx4WAJA60uN9wMyZMzVz5swejznn9Nhjj+mXv/ylZs2aJUn64x//KL/frxdffFE33XTTaY+JRqOKRqOxryORSLwjAQCSUEJ/BtTU1KRgMKjy8vLYPp/Pp7KyMtXX1/f4mJqaGvl8vthWWFiYyJEAAP1UQgMUDAYlSX6/v9t+v98fO/ZN1dXVCofDsa2lpSWRIwEA+qm434JLNI/HI4/HYz0GAOA8S+gVUF5eniQpFAp12x8KhWLHAACQEhyg4uJi5eXlqba2NrYvEolo7969CgQCifxWAIAkF/dbcMePH9eRI0diXzc1NenAgQPKyclRUVGRli5dqgceeECjRo1ScXGx7rvvPhUUFGj27NmJnBsAkOTiDtD+/fv1wx/+MPb18uXLJUnz58/Xpk2bdM8996ijo0MLFy5UW1ubpk2bpp07d2rgwIGJmxoAkPTSnHPOeoj/FolE5PP5FA6H5fV6rccBAMTpbF/H+V1wAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJuIKUE1NjSZNmqQhQ4Zo+PDhmj17thobG7ud09nZqaqqKuXm5mrw4MGqrKxUKBRK6NAAgOQXV4Dq6upUVVWlPXv2aPfu3Tp58qSuvfZadXR0xM5ZtmyZtm/frq1bt6qurk6tra2aM2dOwgcHACS3NOec6+2DP/vsMw0fPlx1dXX6wQ9+oHA4rGHDhmnz5s264YYbJEkffvihRo8erfr6ek2ZMuVbnzMSicjn8ykcDsvr9fZ2NACAkbN9HT+nnwGFw2FJUk5OjiSpoaFBJ0+eVHl5eeyckpISFRUVqb6+vsfniEajikQi3TYAQOrrdYC6urq0dOlSTZ06VWPHjpUkBYNBZWZmKjs7u9u5fr9fwWCwx+epqamRz+eLbYWFhb0dCQCQRHodoKqqKr3//vvasmXLOQ1QXV2tcDgc21paWs7p+QAAySG9Nw9avHixXnnlFb3xxhsaMWJEbH9eXp5OnDihtra2bldBoVBIeXl5PT6Xx+ORx+PpzRgAgCQW1xWQc06LFy/Wtm3b9Oqrr6q4uLjb8dLSUmVkZKi2tja2r7GxUc3NzQoEAomZGACQEuK6AqqqqtLmzZv10ksvaciQIbGf6/h8PmVlZcnn82nBggVavny5cnJy5PV6tWTJEgUCgbO6Aw4AcOGI6zbstLS0Hvdv3LhRP/vZzyR99UHUu+66S88//7yi0agqKiq0bt26M74F903chg0Aye1sX8fP6XNAfYEAAUByOy+fAwIAoLcIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgIm4ArR+/XqNGzdOXq9XXq9XgUBAO3bsiB3v7OxUVVWVcnNzNXjwYFVWVioUCiV8aABA8osrQCNGjNDq1avV0NCg/fv3a/r06Zo1a5Y++OADSdKyZcu0fft2bd26VXV1dWptbdWcOXP6ZHAAQHJLc865c3mCnJwcPfzww7rhhhs0bNgwbd68WTfccIMk6cMPP9To0aNVX1+vKVOmnNXzRSIR+Xw+hcNheb3ecxkNAGDgbF/He/0zoFOnTmnLli3q6OhQIBBQQ0ODTp48qfLy8tg5JSUlKioqUn19/RmfJxqNKhKJdNsAAKkv7gAdPHhQgwcPlsfj0e23365t27ZpzJgxCgaDyszMVHZ2drfz/X6/gsHgGZ+vpqZGPp8vthUWFsa9CABA8ok7QFdccYUOHDigvXv3atGiRZo/f74OHTrU6wGqq6sVDodjW0tLS6+fCwCQPNLjfUBmZqYuu+wySVJpaan27dunxx9/XHPnztWJEyfU1tbW7SooFAopLy/vjM/n8Xjk8XjinxwAkNTO+XNAXV1dikajKi0tVUZGhmpra2PHGhsb1dzcrEAgcK7fBgCQYuK6AqqurtbMmTNVVFSk9vZ2bd68Wa+//rp27doln8+nBQsWaPny5crJyZHX69WSJUsUCATO+g44AMCFI64AHTt2TD/96U919OhR+Xw+jRs3Trt27dKPfvQjSdKjjz6qAQMGqLKyUtFoVBUVFVq3bl2fDA4ASG7n/DmgRONzQACQ3Pr8c0AAAJwLAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwkW49AABYSUtL63G/c+48T3Jh4goIAGCCAAEATBAgAIAJAgQAMEGAAAAmuAsOwAWLu91scQUEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABg4pwCtHr1aqWlpWnp0qWxfZ2dnaqqqlJubq4GDx6syspKhUKhc50TAJBieh2gffv26cknn9S4ceO67V+2bJm2b9+urVu3qq6uTq2trZozZ845DwoASC29CtDx48c1b948Pf300xo6dGhsfzgc1jPPPKPf/va3mj59ukpLS7Vx40b9/e9/1549exI2NAAg+fUqQFVVVbruuutUXl7ebX9DQ4NOnjzZbX9JSYmKiopUX1/f43NFo1FFIpFuGwAg9aXH+4AtW7bonXfe0b59+047FgwGlZmZqezs7G77/X6/gsFgj89XU1OjX/3qV/GOAQBIcnFdAbW0tOjOO+/Uc889p4EDByZkgOrqaoXD4djW0tKSkOcFAPRvcQWooaFBx44d09VXX6309HSlp6errq5Oa9asUXp6uvx+v06cOKG2trZujwuFQsrLy+vxOT0ej7xeb7cNwHmU1sMGnAdxvQU3Y8YMHTx4sNu+W2+9VSUlJbr33ntVWFiojIwM1dbWqrKyUpLU2Nio5uZmBQKBxE0NAEh6cQVoyJAhGjt2bLd9gwYNUm5ubmz/ggULtHz5cuXk5Mjr9WrJkiUKBAKaMmVK4qYGACS9uG9C+DaPPvqoBgwYoMrKSkWjUVVUVGjdunWJ/jYAgCSX5pxz1kP8t0gkIp/Pp3A4zM+DgPOhp5/59KtXBSSbs30d53fBAQBMJPwtOABJhqsdGOEKCABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAm4grQ/fffr7S0tG5bSUlJ7HhnZ6eqqqqUm5urwYMHq7KyUqFQKOFDAziztDNsQH8T9xXQlVdeqaNHj8a2N998M3Zs2bJl2r59u7Zu3aq6ujq1trZqzpw5CR0YAJAa0uN+QHq68vLyTtsfDof1zDPPaPPmzZo+fbokaePGjRo9erT27NmjKVOm9Ph80WhU0Wg09nUkEol3JABAEor7Cujw4cMqKCjQpZdeqnnz5qm5uVmS1NDQoJMnT6q8vDx2bklJiYqKilRfX3/G56upqZHP54tthYWFvVgGACDZxBWgsrIybdq0STt37tT69evV1NSka665Ru3t7QoGg8rMzFR2dna3x/j9fgWDwTM+Z3V1tcLhcGxraWnp1UIAAMklrrfgZs6cGfv3uHHjVFZWppEjR+qFF15QVlZWrwbweDzyeDy9eiwAIHmd023Y2dnZuvzyy3XkyBHl5eXpxIkTamtr63ZOKBTq8WdGAPqGO8MG9DfnFKDjx4/r448/Vn5+vkpLS5WRkaHa2trY8cbGRjU3NysQCJzzoACA1BLXW3B33323rr/+eo0cOVKtra1atWqVLrroIt18883y+XxasGCBli9frpycHHm9Xi1ZskSBQOCMd8ABAC5ccQXoX//6l26++Wb9+9//1rBhwzRt2jTt2bNHw4YNkyQ9+uijGjBggCorKxWNRlVRUaF169b1yeAAgOSW5pzrV28PRyIR+Xw+hcNheb1e63EAAHE629dxfhccAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGAirt+GDcBGWg/7+tVvEQZ6gSsgAIAJAgQAMEGAAAAmCBAAwAQBAgCY4C44IAlwxxtSEVdAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMxB2gTz/9VLfccotyc3OVlZWlq666Svv3748dd85p5cqVys/PV1ZWlsrLy3X48OGEDg0ASH5xBeiLL77Q1KlTlZGRoR07dujQoUN65JFHNHTo0Ng5Dz30kNasWaMNGzZo7969GjRokCoqKtTZ2Znw4QEAySvNOefO9uQVK1borbfe0t/+9rcejzvnVFBQoLvuukt33323JCkcDsvv92vTpk266aabvvV7RCIR+Xw+hcNheb3esx0NANBPnO3reFxXQC+//LImTpyoG2+8UcOHD9eECRP09NNPx443NTUpGAyqvLw8ts/n86msrEz19fU9Pmc0GlUkEum2AQBSX1wB+uSTT7R+/XqNGjVKu3bt0qJFi3THHXfo2WeflSQFg0FJkt/v7/Y4v98fO/ZNNTU18vl8sa2wsLA36wAAJJm4AtTV1aWrr75aDz74oCZMmKCFCxfqtttu04YNG3o9QHV1tcLhcGxraWnp9XMBAJJHXAHKz8/XmDFjuu0bPXq0mpubJUl5eXmSpFAo1O2cUCgUO/ZNHo9HXq+32wYASH1xBWjq1KlqbGzstu+jjz7SyJEjJUnFxcXKy8tTbW1t7HgkEtHevXsVCAQSMC4AIFWkx3PysmXL9P3vf18PPvigfvKTn+jtt9/WU089paeeekqSlJaWpqVLl+qBBx7QqFGjVFxcrPvuu08FBQWaPXt2X8wPAEhScQVo0qRJ2rZtm6qrq/XrX/9axcXFeuyxxzRv3rzYOffcc486Ojq0cOFCtbW1adq0adq5c6cGDhyY8OEBAMkrrs8BnQ98DggAkluffA4IAIBEIUAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMxPXbsM+Hr383aiQSMZ4EANAbX79+f9vvuu53AWpvb5ckFRYWGk8CADgX7e3t8vl8Zzze7/4cQ1dXl1pbWzVkyBC1t7ersLBQLS0tKf2nGSKRCOtMERfCGiXWmWoSvU7nnNrb21VQUKABA878k55+dwU0YMAAjRgxQtJXf2FVkrxeb0r/53+NdaaOC2GNEutMNYlc5/+68vkaNyEAAEwQIACAiX4dII/Ho1WrVsnj8ViP0qdYZ+q4ENYosc5UY7XOfncTAgDgwtCvr4AAAKmLAAEATBAgAIAJAgQAMEGAAAAm+nWA1q5dq+9+97saOHCgysrK9Pbbb1uPdE7eeOMNXX/99SooKFBaWppefPHFbsedc1q5cqXy8/OVlZWl8vJyHT582GbYXqqpqdGkSZM0ZMgQDR8+XLNnz1ZjY2O3czo7O1VVVaXc3FwNHjxYlZWVCoVCRhP3zvr16zVu3LjYJ8cDgYB27NgRO54Ka/ym1atXKy0tTUuXLo3tS4V13n///UpLS+u2lZSUxI6nwhq/9umnn+qWW25Rbm6usrKydNVVV2n//v2x4+f7NajfBujPf/6zli9frlWrVumdd97R+PHjVVFRoWPHjlmP1msdHR0aP3681q5d2+Pxhx56SGvWrNGGDRu0d+9eDRo0SBUVFers7DzPk/ZeXV2dqqqqtGfPHu3evVsnT57Utddeq46Ojtg5y5Yt0/bt27V161bV1dWptbVVc+bMMZw6fiNGjNDq1avV0NCg/fv3a/r06Zo1a5Y++OADSamxxv+2b98+Pfnkkxo3bly3/amyziuvvFJHjx6NbW+++WbsWKqs8YsvvtDUqVOVkZGhHTt26NChQ3rkkUc0dOjQ2Dnn/TXI9VOTJ092VVVVsa9PnTrlCgoKXE1NjeFUiSPJbdu2LfZ1V1eXy8vLcw8//HBsX1tbm/N4PO755583mDAxjh075iS5uro659xXa8rIyHBbt26NnfOPf/zDSXL19fVWYybE0KFD3e9///uUW2N7e7sbNWqU2717t/u///s/d+eddzrnUuf/ctWqVW78+PE9HkuVNTrn3L333uumTZt2xuMWr0H98groxIkTamhoUHl5eWzfgAEDVF5ervr6esPJ+k5TU5OCwWC3Nft8PpWVlSX1msPhsCQpJydHktTQ0KCTJ092W2dJSYmKioqSdp2nTp3Sli1b1NHRoUAgkHJrrKqq0nXXXddtPVJq/V8ePnxYBQUFuvTSSzVv3jw1NzdLSq01vvzyy5o4caJuvPFGDR8+XBMmTNDTTz8dO27xGtQvA/T555/r1KlT8vv93fb7/X4Fg0GjqfrW1+tKpTV3dXVp6dKlmjp1qsaOHSvpq3VmZmYqOzu727nJuM6DBw9q8ODB8ng8uv3227Vt2zaNGTMmpda4ZcsWvfPOO6qpqTntWKqss6ysTJs2bdLOnTu1fv16NTU16ZprrlF7e3vKrFGSPvnkE61fv16jRo3Srl27tGjRIt1xxx169tlnJdm8BvW7P8eA1FFVVaX333+/2/vpqeSKK67QgQMHFA6H9Ze//EXz589XXV2d9VgJ09LSojvvvFO7d+/WwIEDrcfpMzNnzoz9e9y4cSorK9PIkSP1wgsvKCsry3CyxOrq6tLEiRP14IMPSpImTJig999/Xxs2bND8+fNNZuqXV0CXXHKJLrrootPuNAmFQsrLyzOaqm99va5UWfPixYv1yiuv6LXXXov9fSfpq3WeOHFCbW1t3c5PxnVmZmbqsssuU2lpqWpqajR+/Hg9/vjjKbPGhoYGHTt2TFdffbXS09OVnp6uuro6rVmzRunp6fL7/Smxzm/Kzs7W5ZdfriNHjqTM/6Uk5efna8yYMd32jR49OvZ2o8VrUL8MUGZmpkpLS1VbWxvb19XVpdraWgUCAcPJ+k5xcbHy8vK6rTkSiWjv3r1JtWbnnBYvXqxt27bp1VdfVXFxcbfjpaWlysjI6LbOxsZGNTc3J9U6e9LV1aVoNJoya5wxY4YOHjyoAwcOxLaJEydq3rx5sX+nwjq/6fjx4/r444+Vn5+fMv+XkjR16tTTPhLx0UcfaeTIkZKMXoP65NaGBNiyZYvzeDxu06ZN7tChQ27hwoUuOzvbBYNB69F6rb293b377rvu3XffdZLcb3/7W/fuu++6f/7zn84551avXu2ys7PdSy+95N577z03a9YsV1xc7L788kvjyc/eokWLnM/nc6+//ro7evRobPvPf/4TO+f22293RUVF7tVXX3X79+93gUDABQIBw6njt2LFCldXV+eamprce++951asWOHS0tLcX//6V+dcaqyxJ/99F5xzqbHOu+66y73++uuuqanJvfXWW668vNxdcskl7tixY8651Fijc869/fbbLj093f3mN79xhw8fds8995y7+OKL3Z/+9KfYOef7NajfBsg555544glXVFTkMjMz3eTJk92ePXusRzonr732mpN02jZ//nzn3Fe3Qd53333O7/c7j8fjZsyY4RobG22HjlNP65PkNm7cGDvnyy+/dL/4xS/c0KFD3cUXX+x+/OMfu6NHj9oN3Qs///nP3ciRI11mZqYbNmyYmzFjRiw+zqXGGnvyzQClwjrnzp3r8vPzXWZmpvvOd77j5s6d644cORI7ngpr/Nr27dvd2LFjncfjcSUlJe6pp57qdvx8vwbx94AAACb65c+AAACpjwABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgIn/B6qdNWEcUB1VAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for image, label in zip(batch, labels):\n",
    "    plt.imshow(image.permute(2, 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "f7859c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 64])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e23da48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3148654e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052b58b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170a9f62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61278ab5",
   "metadata": {},
   "source": [
    "### VGG19 performance on CalTech101 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "8d57fbf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): ReLU(inplace=True)\n",
       "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): ReLU(inplace=True)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): ReLU(inplace=True)\n",
       "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg19 = torchvision.models.vgg19(weights='DEFAULT')\n",
    "vgg19.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "929ed651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_image_classification(img_tensors, top_k: int = 5):\n",
    "    # compute prediction\n",
    "    output = model(img_tensors)\n",
    "    probs = F.softmax(output)\n",
    "    topk_prob, topk_catid = torch.topk(probs, top_k)\n",
    "    \n",
    "    # convert prediction classes to class name\n",
    "    top_results = []\n",
    "    batch_size = topk_prob.shape[0]\n",
    "    for batch_idx in range(batch_size):\n",
    "        batch_res = []\n",
    "        for prob, catid in zip(topk_prob[batch_idx], topk_catid[batch_idx]):\n",
    "            idx = catid.item()\n",
    "            batch_res.append((idx, dct_imagenet[idx], prob.item()))\n",
    "        top_results.append(batch_res)\n",
    "    \n",
    "    return top_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "408d2473",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "conv2d() received an invalid combination of arguments - got (list, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!list of [Tensor, Tensor]!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!list of [Tensor, Tensor]!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[179], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# TODO\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mget_top_image_classification\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[178], line 3\u001b[0m, in \u001b[0;36mget_top_image_classification\u001b[0;34m(img_tensors, top_k)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_top_image_classification\u001b[39m(img_tensors, top_k: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# compute prediction\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(output)\n\u001b[1;32m      5\u001b[0m     topk_prob, topk_catid \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtopk(probs, top_k)\n",
      "File \u001b[0;32m~/pvenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pvenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[100], line 20\u001b[0m, in \u001b[0;36mVGG19_Caltech.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 20\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n\u001b[1;32m     22\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(x)\n",
      "File \u001b[0;32m~/pvenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pvenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/pvenv/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/pvenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pvenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/pvenv/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pvenv/lib/python3.11/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: conv2d() received an invalid combination of arguments - got (list, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!list of [Tensor, Tensor]!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!list of [Tensor, Tensor]!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, int)\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "get_top_image_classification(batch, top_k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195e90cb",
   "metadata": {},
   "source": [
    "### Fine-Tuning VGG19 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a2c7dfcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7612572",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "51fb0526",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG19_Caltech(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes=101):\n",
    "        super(VGG19_Caltech, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.features = vgg19.features\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e4a9b863",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG19_Caltech()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb935a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TODO: training model with caltech data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35642e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TODO: compare vgg19 performance vs vgg19 performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
